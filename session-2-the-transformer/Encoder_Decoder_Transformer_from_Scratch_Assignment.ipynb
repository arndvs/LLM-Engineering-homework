{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZwKP3f2HGz9"
      },
      "source": [
        "# Encoder-Decoder Transformer Model from Scratch in PyTorch\n",
        "\n",
        "In today's notebook, we'll be focusing on two major components of transformers:\n",
        "\n",
        "1. The Building Blocks\n",
        "2. Training the Model\n",
        "\n",
        "As a brunt of the in-class time will be spent on the building blocks, we'll leave the training logic as an assignment to complete. We'll be focusing more specifically on training once we start using decoder-only architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z43kfJGvlQyA"
      },
      "source": [
        "# The Building Block Fundamentals of Transformer Architecture\n",
        "\n",
        "We're going to start with an example of an encoder-decoder model - the kind found in the classic paper:\n",
        "\n",
        "[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf).\n",
        "\n",
        "We'll walk through each step in code - leveraging the [PyTorch]() library heavily - in order to get an idea of how these models work.\n",
        "\n",
        "While this example notebook could be extended to a sincere usecase - we'll be using a toy dataset, and we will not fully train the model until it converges (under-train), as the full training process might take many days!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sd2KYudl0Hn"
      },
      "source": [
        "## The Desired Architecture\n",
        "\n",
        "![image](https://i.imgur.com/YPjbqW6.png)\n",
        "\n",
        "We'll skip over the diagram for now, and talk through each component in detail!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TPUnyvsuovuP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR8M3oT0l8fM"
      },
      "source": [
        "## Embedding\n",
        "\n",
        "![image](https://i.imgur.com/sFlEZ2e.png)\n",
        "\n",
        "The first step will be do convert our tokenized sequence of inputs into an embedding vector. This allows use to understand a rich amount of information about input sequences and their semantic meanings.\n",
        "\n",
        "As the embedding layer will be training along side the rest of the model - it will allow us to have an excellent vector-representation of the tokens in our dataset.\n",
        "\n",
        "Let's see how it looks in code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ddpQjPJWmXZg"
      },
      "outputs": [],
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "  def __init__(self, d_model: int, vocab_size: int, verbose=False) -> None:\n",
        "    \"\"\"\n",
        "    vocab_size - the size of our vocabulary\n",
        "    d_model - the dimension of our embeddings and the input dimension for our model\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.verbose = verbose\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.verbose:\n",
        "      print(f\"Embedding Vector (1st 5 elements): {self.embedding(x)[:5] * math.sqrt(self.d_model)}\")\n",
        "    return self.embedding(x) * math.sqrt(self.d_model) # scale embeddings by square root of d_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCx8DYkaxwto"
      },
      "source": [
        "### Test Embedding Layer\n",
        "\n",
        "We'll set up a sample Embedding Layer and then test that it does what we'd expect!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xl9VdXuYwfVU"
      },
      "outputs": [],
      "source": [
        "def test_input_embeddings_with_example():\n",
        "    # Create a small embedding layer\n",
        "    embed = InputEmbeddings(d_model=512, vocab_size=1000)\n",
        "\n",
        "    # Example sentence tokens (simplified)\n",
        "    tokens = torch.tensor([[1, 2, 3, 4, 5]])  # \"The cat sat down quickly\"\n",
        "\n",
        "    output = embed(tokens)\n",
        "    print(f\"Input shape: {tokens.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(\"\\nExample shows how words are converted to high-dimensional vectors\")\n",
        "\n",
        "    # Run technical test\n",
        "    assert output.shape == (1, 5, 512), f\"Expected shape (1, 5, 512), got {output.shape}\"\n",
        "    print(\"âœ“ Input Embeddings Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBIElPiqxORa",
        "outputId": "f690bca8-8350-4c27-811c-152b21ac1114"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([1, 5])\n",
            "Output shape: torch.Size([1, 5, 512])\n",
            "\n",
            "Example shows how words are converted to high-dimensional vectors\n",
            "âœ“ Input Embeddings Test Passed\n"
          ]
        }
      ],
      "source": [
        "test_input_embeddings_with_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra5KCa1KnfrC"
      },
      "source": [
        "## Positional Encoding\n",
        "\n",
        "![image](https://i.imgur.com/IIA3NK3.png)\n",
        "\n",
        "We need to impart information about where each token is in the sequence, but we aren't using any recurrence or convolutions - the easiest way to encode positional information is to inject positional information into our input embeddings.\n",
        "\n",
        "We're going to use the process outlined in the paper to do this - which is to use a specific combination of functions to add positional information to the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5K3NXh7MoM5D"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, seq_len: int, dropout: float, verbose=False) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.seq_len = seq_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.verbose=verbose\n",
        "\n",
        "    positional_embeddings = torch.zeros(seq_len, d_model)\n",
        "    positional_sequence_vector = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    positional_model_vector = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    positional_embeddings[:, 0::2] = torch.sin(positional_sequence_vector * positional_model_vector)\n",
        "    positional_embeddings[:, 1::2] = torch.cos(positional_sequence_vector * positional_model_vector)\n",
        "    positional_embeddings = positional_embeddings.unsqueeze(0)\n",
        "\n",
        "    self.register_buffer('positional_embeddings', positional_embeddings)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + (self.positional_embeddings[:, :x.shape[1], :]).requires_grad_(False)\n",
        "    if self.verbose:\n",
        "      print(f\"Positional Encodings (1st 5 elements): {x}\")\n",
        "    return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkcMmoT2yOZs"
      },
      "source": [
        "### Test Positional Encoding Layer\n",
        "\n",
        "We'll set up a sample Positional Encoding Layer and then test that it does what we'd expect!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ayoWMbSYySBp"
      },
      "outputs": [],
      "source": [
        "def test_positional_encoding_with_example():\n",
        "    pos = PositionalEncoding(d_model=512, seq_len=10, dropout=0.1)\n",
        "\n",
        "    # Create sample embeddings for \"The cat sat\"\n",
        "    x = torch.randn(1, 3, 512)\n",
        "\n",
        "    output = pos(x)\n",
        "    print(\"Input tokens position:  [1, 2, 3]\")\n",
        "    print(\"Added position info to each word's embedding\")\n",
        "    print(f\"Output maintains shape: {output.shape}\")\n",
        "\n",
        "    # Verify position information was added\n",
        "    assert not torch.allclose(output, x), \"Position information should modify embeddings\"\n",
        "    print(\"âœ“ Positional Encoding Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4hDXJaGyU5M",
        "outputId": "bc1b382f-b182-4bb3-d452-54610d213f0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input tokens position:  [1, 2, 3]\n",
            "Added position info to each word's embedding\n",
            "Output maintains shape: torch.Size([1, 3, 512])\n",
            "âœ“ Positional Encoding Test Passed\n"
          ]
        }
      ],
      "source": [
        "test_positional_encoding_with_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueiM7LzKpcFY"
      },
      "source": [
        "## Add & Norm\n",
        "\n",
        "Next we'll tackle the Add & Norm Block of the diagram.\n",
        "\n",
        "![image](https://i.imgur.com/otdEq4D.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lDABPLSKqOt"
      },
      "source": [
        "### Layer Normalization\n",
        "\n",
        "The first step is to add layer normalization. You can read more about it [here](https://paperswithcode.com/method/layer-normalization)!\n",
        "\n",
        "The basic idea is that it makes training the model a bit easier, and allows the model to generalize a bit better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1Nlv7BH7ruSf"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self, features: int, epsilon:float=10**-6) -> None:\n",
        "    super().__init__()\n",
        "    self.epsilon = epsilon\n",
        "    self.gamma = nn.Parameter(torch.ones(features))\n",
        "    self.beta = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim = -1, keepdim = True)\n",
        "    standard_deviation = x.std(dim = -1, keepdim = True)\n",
        "    return self.gamma * (x - mean) / (standard_deviation + self.epsilon) + self.beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vft5kFUBy1aE"
      },
      "source": [
        "### Test Layer Normalization\n",
        "\n",
        "We'll set up a sample Layer Normalization and then test that it does what we'd expect!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "01Llr9cwy7C2"
      },
      "outputs": [],
      "source": [
        "def test_layer_normalization_with_example():\n",
        "    layer_norm = LayerNormalization(features=3)  # Smaller feature size for example\n",
        "\n",
        "    # Simulate word embeddings with different magnitudes\n",
        "    word_embeddings = torch.tensor([\n",
        "        [2.5, 4.1, -3.2],  # \"The\" (high magnitude)\n",
        "        [0.1, 0.2, -0.1],  # \"cat\" (low magnitude)\n",
        "        [8.2, -6.1, 5.5]   # \"sat\" (very high magnitude)\n",
        "    ]).unsqueeze(0)\n",
        "\n",
        "    normalized = layer_norm(word_embeddings)\n",
        "\n",
        "    print(\"Before normalization (magnitudes vary greatly):\")\n",
        "    print(word_embeddings[0])\n",
        "    print(\"\\nAfter normalization (values scaled to similar ranges):\")\n",
        "    print(normalized[0])\n",
        "\n",
        "    # Verify statistical properties\n",
        "    mean = normalized.mean(dim=-1)\n",
        "    var = normalized.var(dim=-1)\n",
        "    assert torch.allclose(mean, torch.zeros_like(mean), atol=1e-5)\n",
        "    assert torch.allclose(var, torch.ones_like(var), atol=1e-5)\n",
        "    print(\"âœ“ Layer Normalization Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75hToLCry9mR",
        "outputId": "26d116f9-d379-4328-92c9-f726c8e17dad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before normalization (magnitudes vary greatly):\n",
            "tensor([[ 2.5000,  4.1000, -3.2000],\n",
            "        [ 0.1000,  0.2000, -0.1000],\n",
            "        [ 8.2000, -6.1000,  5.5000]])\n",
            "\n",
            "After normalization (values scaled to similar ranges):\n",
            "tensor([[ 0.3562,  0.7732, -1.1293],\n",
            "        [ 0.2182,  0.8729, -1.0911],\n",
            "        [ 0.7459, -1.1363,  0.3905]], grad_fn=<SelectBackward0>)\n",
            "âœ“ Layer Normalization Test Passed\n"
          ]
        }
      ],
      "source": [
        "test_layer_normalization_with_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwXt7KKKycrl"
      },
      "source": [
        "### Residual Connection\n",
        "\n",
        "Another technique that makes model training easier, we add a Residual connection to the outputs of the Attention Block - this helps to prevent vanishing gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XKwGFD2-yd-Z"
      },
      "outputs": [],
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self, features: int, dropout: float = 0.1) -> None:\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.layernorm = LayerNormalization(features)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    return x + self.dropout(sublayer(self.layernorm(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p668YYgBzc3G"
      },
      "source": [
        "### Testing Residual Connection\n",
        "\n",
        "We'll set up a sample Residual Connection and then test that it does what we'd expect!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "heebV5mIzje4"
      },
      "outputs": [],
      "source": [
        "def test_residual_connection_with_example():\n",
        "    residual = ResidualConnection(features=3, dropout=0.1)\n",
        "\n",
        "    # Original input \"The cat\"\n",
        "    x = torch.tensor([\n",
        "        [1.0, 1.0, 1.0],\n",
        "        [2.0, 2.0, 2.0]\n",
        "    ]).unsqueeze(0)\n",
        "\n",
        "    # Sublayer that makes meaningful changes\n",
        "    def sublayer(x):\n",
        "        return torch.nn.functional.relu(x + 0.5) # Non-linear transformation\n",
        "\n",
        "    output = residual(x, sublayer)\n",
        "\n",
        "    print(\"Original input:\")\n",
        "    print(x[0])\n",
        "    print(\"\\nAfter residual connection (combines original + transformed):\")\n",
        "    print(output[0])\n",
        "\n",
        "    # Verify output changed but maintained shape\n",
        "    assert output.shape == x.shape\n",
        "    assert torch.any(torch.abs(output - x) > 1e-6), \"Output should differ from input\"\n",
        "    print(\"âœ“ Residual Connection Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZMF65cmzmgx",
        "outputId": "e0d20d18-1e5a-4e6b-9167-f8ef460841cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original input:\n",
            "tensor([[1., 1., 1.],\n",
            "        [2., 2., 2.]])\n",
            "\n",
            "After residual connection (combines original + transformed):\n",
            "tensor([[1.5556, 1.5556, 1.5556],\n",
            "        [2.5556, 2.5556, 2.5556]], grad_fn=<SelectBackward0>)\n",
            "âœ“ Residual Connection Test Passed\n"
          ]
        }
      ],
      "source": [
        "test_residual_connection_with_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIOZp3xhsaXK"
      },
      "source": [
        "## Feed Forward Network\n",
        "\n",
        "![image](https://i.imgur.com/woEqBjQ.png)\n",
        "\n",
        "Moving onto the next component, we have our feed forward network.\n",
        "\n",
        "The feed forward networks servers two purposes in our model:\n",
        "\n",
        "1. It reforms the attention outputs into a format that works with the next block.\n",
        "\n",
        "2. It helps add complexity to prevent each attention block acting in a similar fashion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JueNG2UBszbR"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "  def __init__(self, d_model: int, d_ff: int = 2048, dropout: float = 0.1) -> None:\n",
        "    \"\"\"\n",
        "    d_model - dimension of model\n",
        "    d_ff - dimension of feed forward network\n",
        "    dropout - regularization measure\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU-H1m4L0UhY"
      },
      "source": [
        "### Testing the Feed-forward Block\n",
        "\n",
        "Let's test!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pwtbsJou0fxz"
      },
      "outputs": [],
      "source": [
        "def test_feed_forward_block_with_example():\n",
        "   ff_block = FeedForwardBlock(d_model=3, d_ff=8)  # Small dimensions for demonstration\n",
        "\n",
        "   # Input: Word embeddings for \"The cat\"\n",
        "   x = torch.tensor([\n",
        "       [1.0, 0.5, 0.2],  # \"The\"\n",
        "       [2.0, -0.3, 1.1]  # \"cat\"\n",
        "   ]).unsqueeze(0)\n",
        "\n",
        "   output = ff_block(x)\n",
        "\n",
        "   print(\"Input embeddings:\")\n",
        "   print(x[0])\n",
        "   print(\"\\nAfter feed-forward transformation:\")\n",
        "   print(output[0])\n",
        "\n",
        "   # First linear layer expands to d_ff dimensions\n",
        "   # ReLU keeps only positive values\n",
        "   # Second linear layer projects back to d_model dimensions\n",
        "   assert output.shape == x.shape\n",
        "   assert not torch.allclose(output, x)\n",
        "   print(\"âœ“ Feed Forward Block Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMHE9fWB0hj3",
        "outputId": "18878cc8-a9f7-47d0-fea1-6859ad38c8cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input embeddings:\n",
            "tensor([[ 1.0000,  0.5000,  0.2000],\n",
            "        [ 2.0000, -0.3000,  1.1000]])\n",
            "\n",
            "After feed-forward transformation:\n",
            "tensor([[-0.0729, -0.0007, -0.0008],\n",
            "        [ 0.0434,  0.0358,  0.0066]], grad_fn=<SelectBackward0>)\n",
            "âœ“ Feed Forward Block Test Passed\n"
          ]
        }
      ],
      "source": [
        "test_feed_forward_block_with_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ5EGkvdtP0O"
      },
      "source": [
        "## Multi-Head Attention\n",
        "\n",
        "![image](https://i.imgur.com/4qOT46y.png)\n",
        "\n",
        "Next up is the heart and soul of the Transformer - Multi-Head Attention.\n",
        "\n",
        "We'll break it down into the basic building blocks in code in the following section!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Oto52ZyvX0k"
      },
      "source": [
        "### Multi-Head Attention Class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mDHkw1f_vmuv"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model: int = 512, num_heads: int = 8, dropout: float = 0.1) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    assert d_model % num_heads == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEm7I6iwMiom"
      },
      "source": [
        "### ðŸ‘ªâ“Discussion Question 1:\n",
        "\n",
        "Describe, in your own words, intuitively what each of `Q`, `K`, and `V` represent - beyond their names and functions.\n",
        "\n",
        "Discuss among your group!\n",
        "\n",
        "There is no single correct answer - and if you get stuck - feel free to ask your favourite Large Language Model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discussion Answer:\n",
        "\n",
        "Q (Query) is like walking into the library and asking \"I want information about space travel.\"\n",
        "\n",
        "K (Keys) is like the library's catalog system - all those labels and categories that help find books. When you ask about space travel, the system checks book titles, categories, and tags to find matches.\n",
        "\n",
        "V (Values) is the actual content in the matching books. Once the system finds relevant books using the catalog (Keys), you get access to the real information inside them (Values).\n",
        "\n",
        "Simple example:\n",
        "\n",
        "You: \"I need info about rockets\" (Query) Library system: Checks categories like \"Space\", \"Engineering\", \"NASA\" (Keys) You get: The actual books and their contents about rockets (Values)\n",
        "\n",
        "It's just matching your question (Q) with the right labels (K) to get you the actual information you need (V). Just like a librarian helps connect your question to the right books, attention helps the computer connect each piece of information to other relevant pieces it needs to understand!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dp3eM7H1cRl"
      },
      "source": [
        "### Testing Multi-Head Attention\n",
        "\n",
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyD7nAM6tb0K"
      },
      "source": [
        "### Scaled Dot-Product Attention\n",
        "\n",
        "![image](https://i.imgur.com/Yp48DuB.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dKk08SvowJIc"
      },
      "outputs": [],
      "source": [
        "def attention(query, key, value, mask, d_k, dropout: nn.Dropout = None):\n",
        "  attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "  if mask is not None:\n",
        "    attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "  attention_scores = attention_scores.softmax(dim=-1)\n",
        "\n",
        "  if dropout is not None:\n",
        "    attention_scores = dropout(attention_scores)\n",
        "\n",
        "  return (attention_scores @ value), attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qphuqoXl-1jW"
      },
      "source": [
        "### â“Question 1:\n",
        "\n",
        "Describe the above code that defines the attension mechanism.\n",
        "\n",
        "Write, in natural language, what each step is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.   First line: attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "*   This is calculating similarity scores between the query and keys\n",
        "*   The @ symbol means matrix multiplication\n",
        "*   We transpose the key matrix to make the multiplication dimensions match\n",
        "*   We divide by square root of d_k to prevent the scores from getting too extreme (too large or too small) - this helps keep training stable\n",
        "*   Think of it like: \"How well does each query match with each key?\"\n",
        "\n",
        "\n",
        "2.   Mask handling: attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "*   If a mask is provided, this step blocks out certain connections we don't want\n",
        "*   It replaces those positions with a very negative number (-1e9)\n",
        "*   This is often used in tasks like translation to prevent the model from \"looking at future words\" when it shouldn't\n",
        "*   Think of it like: \"Don't pay attention to these specific parts\"\n",
        "\n",
        "\n",
        "3. Softmax: attention_scores.softmax(dim=-1)\n",
        "\n",
        "*   This converts our scores into probabilities that add up to 1\n",
        "*   Higher scores become higher probabilities\n",
        "*   Very negative numbers (like our masked values) become nearly zero\n",
        "*   Think of it like: \"Convert scores into percentages of attention\"\n",
        "\n",
        "\n",
        "4. Dropout: attention_scores = dropout(attention_scores)\n",
        "\n",
        "*   If dropout is provided, randomly zeros out some of the attention scores\n",
        "*   This helps prevent overfitting during training\n",
        "*   Think of it like: \"Randomly ignore some connections to make learning more robust\"\n",
        "\n",
        "\n",
        "5. Final calculation: return (attention_scores @ value), attention_scores\n",
        "\n",
        "*   Multiplies the attention probabilities with the values\n",
        "*   This creates a weighted sum of the values, based on how much attention each got\n",
        "*   Also returns the attention scores themselves (useful for visualization or analysis)\n",
        "Think of it like: \"Combine the values based on how much attention each one got\"\n",
        "\n",
        "\n",
        "\n",
        "The whole process is similar to a weighted average, where the weights (attention scores) are calculated based on the relationship between queries and keys, and then applied to the values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac8k7b3SxKOC"
      },
      "source": [
        "### Forward Method\n",
        "\n",
        "This is code is required to do a forward pass with our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "um2Oxv1axWYK"
      },
      "outputs": [],
      "source": [
        "def forward(self, query, key, value, mask):\n",
        "  query = self.w_q(query)\n",
        "  key = self.w_k(key)\n",
        "  value = self.w_v(value)\n",
        "\n",
        "  query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "  key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "  value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "  x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "  x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.d_k)\n",
        "\n",
        "  return self.w_o(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgr_D0mmyEgp"
      },
      "source": [
        "### Combining it All Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "odYKdmwMyH4P"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model: int = 512, num_heads: int = 8, dropout: float = 0.1) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    assert d_model % num_heads == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query, key, value, mask, dropout: nn.Dropout = None):\n",
        "    d_k = query.shape[-1]\n",
        "\n",
        "    attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "    attention_scores = attention_scores.softmax(dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "\n",
        "    return (attention_scores @ value), attention_scores\n",
        "\n",
        "  def forward(self, query, key, value, mask):\n",
        "    query = self.w_q(query)\n",
        "    key = self.w_k(key)\n",
        "    value = self.w_v(value)\n",
        "\n",
        "    query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "    key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "    value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.d_k)\n",
        "\n",
        "    return self.w_o(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySBbU7Zo_x5T"
      },
      "source": [
        "### ðŸ‘ªâ“Discussion Question 2:\n",
        "\n",
        "Work with your breakout room to build a visualization of the above `forward` pass - including dummy inputs.\n",
        "\n",
        "You can use drawing programs (like [Excalidraw](https://excalidraw.com/)), or write a visualization in code.\n",
        "\n",
        "> NOTE: LLMs like Claude 3.5 Sonnet is a fantastic tool to produce (and test) visualizations in it's [Web UI](https://claude.ai/new)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt text](./MultiHeadAttentionForwardPassFlow.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRFqoZyD1-AU"
      },
      "source": [
        "### Testing MultiHeadAttention\n",
        "\n",
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sY1N60di2CHQ"
      },
      "outputs": [],
      "source": [
        "def test_attention_mechanism():\n",
        "   mha = MultiHeadAttention(d_model=6, num_heads=2)\n",
        "\n",
        "   # Simple sequence: \"The cat sleeps\"\n",
        "   seq = torch.tensor([\n",
        "       [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
        "       [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],\n",
        "       [0.0, 0.0, 0.0, 0.0, 1.0, 1.0]\n",
        "   ]).unsqueeze(0)  # [1, 3, 6]\n",
        "\n",
        "   # Mask shape needs to match attention scores [batch, heads, seq_len, seq_len]\n",
        "   attention_scores = torch.ones(1, 2, 3, 3)  # 2 heads, sequence length 3\n",
        "\n",
        "   print(\"Input sequence shape:\", seq.shape)\n",
        "   print(\"Input values (each row is a word):\")\n",
        "   print(seq[0])\n",
        "\n",
        "   output = mha(seq, seq, seq, attention_scores)\n",
        "   print(\"\\nOutput after attention:\")\n",
        "   print(output[0])\n",
        "\n",
        "   # Verify output maintains shape but changes values\n",
        "   assert output.shape == seq.shape\n",
        "   assert not torch.allclose(output, seq)\n",
        "   print(\"âœ“ Multi-Head Attention Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qEqn4BS2DHt",
        "outputId": "90650495-7708-435b-d6b1-676c7625cf59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input sequence shape: torch.Size([1, 3, 6])\n",
            "Input values (each row is a word):\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 1.]])\n",
            "\n",
            "Output after attention:\n",
            "tensor([[ 0.1261,  0.1917, -0.2103, -0.1390, -0.0412, -0.1155],\n",
            "        [ 0.1263,  0.1911, -0.2100, -0.1215, -0.0313, -0.1108],\n",
            "        [ 0.1848,  0.1167, -0.1615, -0.2570, -0.1381, -0.1417]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "âœ“ Multi-Head Attention Test Passed\n"
          ]
        }
      ],
      "source": [
        "test_attention_mechanism()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkKjLhpiyz5b"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "When we pass information through our model - the first thing we will do is Encode it by passing it through our Encoder Blocks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-0edIfMzijj"
      },
      "source": [
        "### Encoder Block\n",
        "\n",
        "![image](https://i.imgur.com/nwNYZAT.png)\n",
        "\n",
        "The encoder takes in the source language sentence (e.g. English). Each word is converted into a vector representation using an embedding layer. Then a positional encoder adds information about the position of each word. This goes through multiple self-attention layers, where each word vector attends to all other word vectors to build contextual representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dMVnZiGDy1PG"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, features: int, self_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x, input_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, input_mask))\n",
        "    x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_twqqReS28ul"
      },
      "source": [
        "### Testing the EncoderBlock\n",
        "\n",
        "Testing time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "PP_wq0R02_g0"
      },
      "outputs": [],
      "source": [
        "def test_encoder_block():\n",
        "   # Create encoder block with small dimensions\n",
        "   mha = MultiHeadAttention(d_model=6, num_heads=2)\n",
        "   ff = FeedForwardBlock(d_model=6, d_ff=12)\n",
        "   encoder = EncoderBlock(features=6, self_attention_block=mha, feed_forward_block=ff, dropout=0.1)\n",
        "\n",
        "   # Input: \"The cat sleeps\"\n",
        "   x = torch.tensor([\n",
        "       [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # \"The\"\n",
        "       [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],  # \"cat\"\n",
        "       [0.0, 0.0, 0.0, 0.0, 1.0, 1.0]   # \"sleeps\"\n",
        "   ]).unsqueeze(0)\n",
        "\n",
        "   # Attention mask\n",
        "   mask = torch.ones(1, 2, 3, 3)  # Allow all connections\n",
        "\n",
        "   output = encoder(x, mask)\n",
        "\n",
        "   print(\"Input sequence:\")\n",
        "   print(x[0])\n",
        "   print(\"\\nAfter encoder processing (self-attention + feed-forward):\")\n",
        "   print(output[0])\n",
        "\n",
        "   assert output.shape == x.shape\n",
        "   assert not torch.allclose(output, x)\n",
        "   print(\"âœ“ Encoder Block Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPAH-NKA3AEN",
        "outputId": "b6d3cec8-4c2e-4fd2-9e33-3f33fc560880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input sequence:\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 1.]])\n",
            "\n",
            "After encoder processing (self-attention + feed-forward):\n",
            "tensor([[ 0.8265,  1.2406, -0.1271, -0.0659, -0.6119, -0.0186],\n",
            "        [-0.1369, -0.4110,  0.8684,  0.6885, -0.3229, -0.1054],\n",
            "        [ 0.0128,  0.0425, -1.1663, -0.0138,  0.6647,  1.1998]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "âœ“ Encoder Block Test Passed\n"
          ]
        }
      ],
      "source": [
        "test_encoder_block()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-AvnoPrzvwu"
      },
      "source": [
        "### Encoder Stack\n",
        "\n",
        "Following along from the original paper - we will organize these blocks into a set of 6.\n",
        "\n",
        "These 6 Encoder Blocks (each with 8 Attention Heads) will comprise our Encoding Stack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "kOaR5SjUzxv7"
      },
      "outputs": [],
      "source": [
        "class EncoderStack(nn.Module):\n",
        "  def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization(features)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQyujsRTz5NT"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "Next, we will take the encoded sequence and decode it through our Decoder Blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBYgl77Kz6bx"
      },
      "source": [
        "### Decoder Block\n",
        "\n",
        "![image](https://i.imgur.com/HtAAXZc.png)\n",
        "\n",
        "The decoder takes in the target language sentence (e.g. Italian). It also converts words to vectors and adds positional info. Then it goes through self-attention layers. Here, a mask is applied so each word can only see the words before it, not after.\n",
        "\n",
        "The decoder also does attention over the encoder output. This allows each French word to find relevant connections with the English words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SIwafbqzz5-n"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, features: int, self_attention_block: MultiHeadAttention, cross_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.cross_attention_block = cross_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "  def forward(self, x, encoder_output, input_mask, target_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, target_mask))\n",
        "    x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, input_mask))\n",
        "    x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0KkwjkM3xe7"
      },
      "source": [
        "### Testing DecoderBlock\n",
        "\n",
        "You know what's up next...testing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UnmnWAVl307S"
      },
      "outputs": [],
      "source": [
        "def test_decoder_block():\n",
        "   # Initialize components with small dimensions\n",
        "   self_attn = MultiHeadAttention(d_model=6, num_heads=2)\n",
        "   cross_attn = MultiHeadAttention(d_model=6, num_heads=2)\n",
        "   ff = FeedForwardBlock(d_model=6, d_ff=12)\n",
        "   decoder = DecoderBlock(features=6, self_attention_block=self_attn,\n",
        "                         cross_attention_block=cross_attn,\n",
        "                         feed_forward_block=ff, dropout=0.1)\n",
        "\n",
        "   # Input: \"El gato\" (target sequence)\n",
        "   x = torch.tensor([\n",
        "       [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # \"El\"\n",
        "       [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],  # \"gato\"\n",
        "   ]).unsqueeze(0)\n",
        "\n",
        "   # Encoder output: \"The cat\" (source sequence)\n",
        "   encoder_output = torch.tensor([\n",
        "       [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # \"The\"\n",
        "       [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],  # \"cat\"\n",
        "   ]).unsqueeze(0)\n",
        "\n",
        "   # Masks\n",
        "   src_mask = torch.ones(1, 2, 2, 2)  # Can attend to all encoder outputs\n",
        "   tgt_mask = torch.tril(torch.ones(1, 2, 2, 2))  # Can only attend to previous words\n",
        "\n",
        "   output = decoder(x, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "   print(\"Input target sequence:\")\n",
        "   print(x[0])\n",
        "   print(\"\\nSource (encoder) sequence:\")\n",
        "   print(encoder_output[0])\n",
        "   print(\"\\nDecoder output (after self-attention, cross-attention, and feed-forward):\")\n",
        "   print(output[0])\n",
        "\n",
        "   assert output.shape == x.shape\n",
        "   assert not torch.allclose(output, x)\n",
        "   print(\"âœ“ Decoder Block Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COJlCq3033A2",
        "outputId": "08f4dc7c-3025-4f3b-eb7c-aaf44ca76ab1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input target sequence:\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.]])\n",
            "\n",
            "Source (encoder) sequence:\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.]])\n",
            "\n",
            "Decoder output (after self-attention, cross-attention, and feed-forward):\n",
            "tensor([[ 1.1330,  1.4736,  0.1730, -0.2102,  0.3285,  0.0476],\n",
            "        [ 0.0510,  0.4641,  1.5059,  1.1527, -0.1152,  0.1938]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "âœ“ Decoder Block Test Passed\n"
          ]
        }
      ],
      "source": [
        "test_decoder_block()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa4yiTNn0BkA"
      },
      "source": [
        "### Decoder Stack\n",
        "\n",
        "We'll use the same number of Decoder Blocks as we did Encoder Blocks - leaving us with 6 Deocder Blocks in our Decoder Stack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1QUXzOXk0CcT"
      },
      "outputs": [],
      "source": [
        "class DecoderStack(nn.Module):\n",
        "  def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization(features)\n",
        "\n",
        "  def forward(self, x, encoder_output, input_mask, target_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, encoder_output, input_mask, target_mask)\n",
        "    return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRFiAP580S4b"
      },
      "source": [
        "## Linear Projection Layer\n",
        "\n",
        "After the decoder's self-attention and encoder-decoder attention layers, we have a context vector representing each Italian word. This context vector has a high dimension (e.g. 512 or 1024).\n",
        "\n",
        "We want to take this context vector and generate a probability distribution over the French vocabulary so we can pick the next translated word.\n",
        "\n",
        "The linear projection layer helps with this. It projects the context vector into a much larger vector called the vocabulary distribution - one entry per word in the vocabulary.\n",
        "\n",
        "For example, if our Italian vocabulary has 50,000 words, the vocabulary distribution will have 50,000 dimensions. Each dimension corresponds to the probability of that Italian word being the correct translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tkBBMAZK0WLB"
      },
      "outputs": [],
      "source": [
        "class LinearProjectionLayer(nn.Module):\n",
        "  def __init__(self, d_model, vocab_size) -> None:\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x) -> None:\n",
        "    return self.proj(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9ucsRWs0lG9"
      },
      "source": [
        "## The Transformer\n",
        "\n",
        "At this point, all we need to do is create a class that represents our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5ip11mmQ0nMM"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder: EncoderBlock, decoder: DecoderBlock, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: LinearProjectionLayer) -> None:\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.tgt_embed = tgt_embed\n",
        "    self.src_pos = src_pos\n",
        "    self.tgt_pos = tgt_pos\n",
        "    self.projection_layer = projection_layer\n",
        "\n",
        "  def encode(self, src, src_mask):\n",
        "    src = self.src_embed(src)\n",
        "    src = self.src_pos(src)\n",
        "    return self.encoder(src, src_mask)\n",
        "\n",
        "  def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "    tgt = self.tgt_embed(tgt)\n",
        "    tgt = self.tgt_pos(tgt)\n",
        "    return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "  def project(self, x):\n",
        "    return self.projection_layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ssr5nA039--"
      },
      "source": [
        "## Building Our Transformer\n",
        "\n",
        "Now that we have each of our components - we need to construct an actual model!\n",
        "\n",
        "We'll use this helper function to aid in our goal and set up our Encoder/Decoder Stacks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kdCt3wNi4EvY"
      },
      "outputs": [],
      "source": [
        "def build_transformer(input_vocab_size: int, target_vocab_size: int, input_seq_len: int, target_seq_len: int, d_model: int=512, N: int=6, num_heads: int=8, dropout: float=0.1, d_ff: int=2048, verbose=True) -> Transformer:\n",
        "  input_embeddings = InputEmbeddings(d_model, input_vocab_size, verbose=verbose)\n",
        "  target_embeddings = InputEmbeddings(d_model, target_vocab_size)\n",
        "\n",
        "  input_position = PositionalEncoding(d_model, input_seq_len, dropout, verbose=verbose)\n",
        "  target_position = PositionalEncoding(d_model, target_seq_len, dropout)\n",
        "\n",
        "  encoder_blocks = []\n",
        "\n",
        "  for _ in range(N):\n",
        "    encoder_self_attention_block = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "    feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "    encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "    encoder_blocks.append(encoder_block)\n",
        "\n",
        "  decoder_blocks = []\n",
        "\n",
        "  for _ in range(N):\n",
        "    decoder_self_attention_block = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "    decoder_cross_attention_block = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "    feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "    decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "    decoder_blocks.append(decoder_block)\n",
        "\n",
        "  encoder_stack = EncoderStack(d_model, nn.ModuleList(encoder_blocks))\n",
        "  decoder_stack = DecoderStack(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "  linear_projection_layer = LinearProjectionLayer(d_model, target_vocab_size)\n",
        "\n",
        "  transformer = Transformer(encoder_stack, decoder_stack, input_embeddings, target_embeddings, input_position, target_position, linear_projection_layer)\n",
        "\n",
        "  for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "  return transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2jfvmRx4ZNd"
      },
      "source": [
        "# Training Our Transformer!\n",
        "\n",
        "We will be using the resources created in [this](https://github.com/hkproj/pytorch-transformer/tree/main) repository to train our model on a English -> French translation task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxF6JLcg5LJp"
      },
      "source": [
        "## Dataset Creation\n",
        "\n",
        "The BilingualDataset is a custom PyTorch dataset for working with translation data. It needs a tokenizer for each language, a dataset of sentence pairs, info on which languages are source and target, and the max sequence length.\n",
        "\n",
        "This class handles tokenizing the sentences, padding them to be the same length, and getting the data into the right format for sequence-to-sequence models. It adds special start, end, and padding tokens so all the inputs and outputs are the same length.\n",
        "\n",
        "When you grab a sample from the dataset, it tokenizes the source and target sentences, pads them, and creates the input tensors the model needs - encoder input, decoder input, and target labels. It also makes masks to show what's real data vs padding, and to make sure the decoder predictions only use previous tokens, not future ones.\n",
        "\n",
        "The BilingualDataset gets the data ready for training seq2seq models in a way that works with the sequential nature of language. The model can only predict the next token based on what came before it, not after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "WLnUZRgk7S-G"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BilingualDataset(Dataset):\n",
        "  def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
        "    super().__init__()\n",
        "    self.seq_len = seq_len\n",
        "\n",
        "    self.ds = ds\n",
        "    self.tokenizer_src = tokenizer_src\n",
        "    self.tokenizer_tgt = tokenizer_tgt\n",
        "    self.src_lang = src_lang\n",
        "    self.tgt_lang = tgt_lang\n",
        "\n",
        "    self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
        "    self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
        "    self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ds)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    src_target_pair = self.ds[idx]\n",
        "    src_text = src_target_pair['translation'][self.src_lang]\n",
        "    tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "    enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "    dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "    enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
        "    dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "\n",
        "    if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "        raise ValueError(\"Sentence is too long\")\n",
        "\n",
        "    encoder_input = torch.cat(\n",
        "        [\n",
        "            self.sos_token,\n",
        "            torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "            self.eos_token,\n",
        "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
        "        ],\n",
        "        dim=0,\n",
        "    )\n",
        "\n",
        "    decoder_input = torch.cat(\n",
        "        [\n",
        "            self.sos_token,\n",
        "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "        ],\n",
        "        dim=0,\n",
        "    )\n",
        "\n",
        "    label = torch.cat(\n",
        "        [\n",
        "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "            self.eos_token,\n",
        "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "        ],\n",
        "        dim=0,\n",
        "    )\n",
        "\n",
        "    assert encoder_input.size(0) == self.seq_len\n",
        "    assert decoder_input.size(0) == self.seq_len\n",
        "    assert label.size(0) == self.seq_len\n",
        "\n",
        "    return {\n",
        "        \"encoder_input\": encoder_input,\n",
        "        \"decoder_input\": decoder_input,\n",
        "        \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
        "        \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
        "        \"label\": label,\n",
        "        \"src_text\": src_text,\n",
        "        \"tgt_text\": tgt_text,\n",
        "    }\n",
        "\n",
        "def causal_mask(size):\n",
        "  mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "  return mask == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0hSYI7F8IAI"
      },
      "source": [
        "## Build Tokenizer For Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8mzAqcVMxk5",
        "outputId": "6558230f-f67d-49a9-b2ce-4a89c957d8dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: fsspec 2024.9.0\n",
            "Uninstalling fsspec-2024.9.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/fsspec-2024.9.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/fsspec/*\n",
            "Proceed (Y/n)? Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/uninstall.py\", line 106, in run\n",
            "    uninstall_pathset = req.uninstall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_install.py\", line 722, in uninstall\n",
            "    uninstalled_pathset.remove(auto_confirm, verbose)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_uninstall.py\", line 364, in remove\n",
            "    if auto_confirm or self._allowed_to_proceed(verbose):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_uninstall.py\", line 404, in _allowed_to_proceed\n",
            "    return ask(\"Proceed (Y/n)? \", (\"y\", \"n\", \"\")) != \"n\"\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/misc.py\", line 235, in ask\n",
            "    response = input(message)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall fsspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZmQ-Mm0NEVt",
        "outputId": "a0687e8e-cdbf-42c6-e09e-b52fab53f08f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fsspec==2024.10.0\n",
            "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/179.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m174.1/179.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.9.0\n",
            "    Uninstalling fsspec-2024.9.0:\n",
            "      Successfully uninstalled fsspec-2024.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 3.1.0 requires fsspec[http]<=2024.9.0,>=2023.1.0, but you have fsspec 2024.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2024.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fsspec==2024.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "QfmUwnpo8qou"
      },
      "outputs": [],
      "source": [
        "!pip install transformers tokenizers datasets -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TCDOGq0UD1W"
      },
      "source": [
        "This will grab all the sentences from our dataset per language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "iAUUEXFz8eGH"
      },
      "outputs": [],
      "source": [
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds:\n",
        "        yield item['translation'][lang]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUDKOCTVUIa7"
      },
      "source": [
        "We'll quickly train a tokenizer on our dataset for both our source and target languages.\n",
        "\n",
        "We'll be sure to add the `[UNK]`, `[PAD]`, `[SOS]`, and `[EOS]` special tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "4s1ZHzKb8hTN"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "def build_tokenizer(config, ds, lang):\n",
        "  tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "  tokenizer.pre_tokenizer = Whitespace()\n",
        "  trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
        "  tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "  return tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEUDihkcVRq1"
      },
      "source": [
        "Now we can create our dataset in a format that our model expects and can train with!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "yI7xXnLo84cE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "def get_ds(config):\n",
        "  # It only has the train split, so we divide it overselves\n",
        "  ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
        "\n",
        "  # Build tokenizers\n",
        "  tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "  tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "  # Keep 90% for training, 10% for validation\n",
        "  train_ds_size = int(0.9 * len(ds_raw))\n",
        "  val_ds_size = len(ds_raw) - train_ds_size\n",
        "  train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "  train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "  val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "  # Find the maximum length of each sentence in the source and target sentence\n",
        "  max_len_src = 0\n",
        "  max_len_tgt = 0\n",
        "\n",
        "  for item in ds_raw:\n",
        "    src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "    tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "    max_len_src = max(max_len_src, len(src_ids))\n",
        "    max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "  print(f'Max length of source sentence: {max_len_src}')\n",
        "  print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "\n",
        "  train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
        "  val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "  return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr3Ys-ufVW1E"
      },
      "source": [
        "We can build our model with this helper function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "FK6k5X829JOo"
      },
      "outputs": [],
      "source": [
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "  model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'], verbose=False)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "JCALGXpf9tnv"
      },
      "outputs": [],
      "source": [
        "def get_weights_file_path(config, epoch: str):\n",
        "  model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
        "  model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
        "  return str(Path('.') / model_folder / model_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "QPWZgCwD9vVX"
      },
      "outputs": [],
      "source": [
        "def latest_weights_file_path(config):\n",
        "  model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
        "  model_filename = f\"{config['model_basename']}*\"\n",
        "  weights_files = list(Path(model_folder).glob(model_filename))\n",
        "  if len(weights_files) == 0:\n",
        "      return None\n",
        "  weights_files.sort()\n",
        "  return str(weights_files[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNjAS-LvV557"
      },
      "source": [
        "Finally....our training loop!\n",
        "\n",
        "We'll spend more time in following weeks discussing this - for now, we'll quickly walk through what's happening:\n",
        "\n",
        "1. Configure the training device (GPU/CPU) and print details. Set device in PyTorch.\n",
        "\n",
        "2. Create directory for saving model weights based on config.\n",
        "\n",
        "3. Get data loaders, tokenizers, and model. Move model to configured device.\n",
        "\n",
        "4. Initialize Adam optimizer with learning rate and epsilon from config.\n",
        "\n",
        "5. Set up initial training parameters like start epoch and global step.\n",
        "\n",
        "6. Define cross-entropy loss function with label smoothing, ignoring padding.\n",
        "\n",
        "---\n",
        "\n",
        "- Main training loop over epochs:\n",
        "\n",
        "  - Clear cache, set model to train mode, initialize progress bar.\n",
        "\n",
        "  - For each batch:\n",
        "\n",
        "    - Move data to device, run model forward/backward passes.\n",
        "    - Compute loss, backprop, update model weights.\n",
        "    - Increment global step.\n",
        "  - After each epoch, save model and optimizer checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "gL1bH7is9OfS"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def train_model(config):\n",
        "  # Define the device\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
        "  print(\"Using device:\", device)\n",
        "  if (device == 'cuda'):\n",
        "    print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "    print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
        "  else:\n",
        "    print(\"Please ensure you're in a GPU enabled Colab Notebook instance.\")\n",
        "  device = torch.device(device)\n",
        "\n",
        "  # Make sure the weights folder exists\n",
        "  Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "  model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "\n",
        "  initial_epoch = 0\n",
        "  global_step = 0\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
        "\n",
        "  for epoch in range(initial_epoch, config['num_epochs']):\n",
        "    torch.cuda.empty_cache()\n",
        "    model.train()\n",
        "    batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "    for batch in batch_iterator:\n",
        "      encoder_input = batch['encoder_input'].to(device)\n",
        "      decoder_input = batch['decoder_input'].to(device)\n",
        "      encoder_mask = batch['encoder_mask'].to(device)\n",
        "      decoder_mask = batch['decoder_mask'].to(device)\n",
        "\n",
        "      encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "      decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "      proj_output = model.project(decoder_output)\n",
        "\n",
        "      label = batch['label'].to(device)\n",
        "\n",
        "      loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "      batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "      global_step += 1\n",
        "\n",
        "    model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
        "    torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'global_step': global_step\n",
        "    }, model_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "f21pxGoS9-gM"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "  \"batch_size\": 64,\n",
        "  \"num_epochs\": 6,\n",
        "  \"lr\": 1e-4,\n",
        "  \"seq_len\": 350,\n",
        "  \"d_model\": 512,\n",
        "  \"datasource\": 'opus_books',\n",
        "  \"lang_src\": \"en\",\n",
        "  \"lang_tgt\": \"it\",\n",
        "  \"model_folder\": \"trained__en_it_translation_model\",\n",
        "  \"model_basename\": \"encoder_decoder_model_\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408,
          "referenced_widgets": [
            "fa87247f7e2a4f8db2fa81d24479503b",
            "450369d045444bd882f937a2e3360633",
            "2ebde74bdf0f47bc8f0e765c923d0617",
            "7c34fa2e87e145a2a081157b53461876",
            "7c7890ba428744a8b73ca27d4df978dc",
            "6870c43752c64d28aa4719f9040adf2e",
            "5000d2f20c8940ccbb48882ec044f794",
            "30fdc2dcc66e45249784d589fc3f6fe4",
            "a3bed5d7ec2b4e4592e53737680df900",
            "0c374a73f8b044ccaf1496600033f4ae",
            "3159e6a7df704ced88e0f7cc827317ae",
            "8a7cb2f19965446b8a0e4a146458b687",
            "3f715be0422440a385277b9ac3281166",
            "7c2499083c47434fbe08b91c51c8a043",
            "914bd3f816e44a2dbc752dd03785a75e",
            "35e03fc3caca4f598b58596284a97268",
            "23d9390da58c40c9960d4061d738281a",
            "5d8d4f2bea7d48179265bd04fe67e0de",
            "d9d0952781ec4d7bae15e7676c5288ac",
            "c4361021b3194e5fb9c2eb05cc390310",
            "8aaae6369779418ab1923cf678182884",
            "b08f48cdd7ec4d9d831ebb71d63d8b9c",
            "6085009c74a6457399a8ac54ce72727e",
            "6d7e11b9cbec499388135ba6eb4dfcdd",
            "cc7141a44ec44a66994c4b3a3386df6b",
            "4688b7bcc6be45d1a11a2bd6de4f5e27",
            "ed08816530a94263a8d10890d4514c16",
            "a8475c0c2a5b48c98088ee59998dde99",
            "ef4db5c750354ea796c24f3085a83325",
            "9691ca93db0846d58e5f69ab63de8e27",
            "dcf3b34b34164aa493154bb080d42dd9",
            "f97814858bac42bd893911fe8e7299e3",
            "3051ca72ff394a43b13dd6d3c464d8ee"
          ]
        },
        "collapsed": true,
        "id": "hfBcSHUo-MU6",
        "outputId": "3765fa19-fef4-4231-8502-a390b99d608b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Device name: NVIDIA A100-SXM4-40GB\n",
            "Device memory: 39.56427001953125 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa87247f7e2a4f8db2fa81d24479503b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a7cb2f19965446b8a0e4a146458b687",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/5.73M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6085009c74a6457399a8ac54ce72727e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of source sentence: 309\n",
            "Max length of target sentence: 274\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 00: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 455/455 [05:28<00:00,  1.38it/s, loss=6.228]\n",
            "Processing Epoch 01: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 455/455 [05:27<00:00,  1.39it/s, loss=5.872]\n",
            "Processing Epoch 02: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 455/455 [05:27<00:00,  1.39it/s, loss=5.584]\n",
            "Processing Epoch 03: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 455/455 [05:27<00:00,  1.39it/s, loss=5.130]\n",
            "Processing Epoch 04: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 455/455 [05:27<00:00,  1.39it/s, loss=5.017]\n",
            "Processing Epoch 05: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 455/455 [05:27<00:00,  1.39it/s, loss=5.072]\n"
          ]
        }
      ],
      "source": [
        "train_model(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT_QNpmqzAxe",
        "outputId": "323a977f-62e8-4ae2-b162-d626892ddd4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of source sentence: 309\n",
            "Max length of target sentence: 274\n",
            "Loading weights from opus_books_trained__en_it_translation_model/encoder_decoder_model_05.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-49-f81f168b8be7>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(model_filename)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): EncoderStack(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x EncoderBlock(\n",
              "        (self_attention_block): MultiHeadAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward_block): FeedForwardBlock(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (residual_connections): ModuleList(\n",
              "          (0-1): 2 x ResidualConnection(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (layernorm): LayerNormalization()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNormalization()\n",
              "  )\n",
              "  (decoder): DecoderStack(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x DecoderBlock(\n",
              "        (self_attention_block): MultiHeadAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (cross_attention_block): MultiHeadAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward_block): FeedForwardBlock(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (residual_connections): ModuleList(\n",
              "          (0-2): 3 x ResidualConnection(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (layernorm): LayerNormalization()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNormalization()\n",
              "  )\n",
              "  (src_embed): InputEmbeddings(\n",
              "    (embedding): Embedding(15698, 512)\n",
              "  )\n",
              "  (tgt_embed): InputEmbeddings(\n",
              "    (embedding): Embedding(22463, 512)\n",
              "  )\n",
              "  (src_pos): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (tgt_pos): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (projection_layer): LinearProjectionLayer(\n",
              "    (proj): Linear(in_features=512, out_features=22463, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_model(config):\n",
        "    # Get dataloaders and tokenizers\n",
        "    _, _, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "\n",
        "    # Initialize model\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size())\n",
        "\n",
        "    # Load trained weights\n",
        "    model_filename = latest_weights_file_path(config)\n",
        "    if model_filename:\n",
        "        print(f\"Loading weights from {model_filename}\")\n",
        "        state = torch.load(model_filename)\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    return model, tokenizer_src, tokenizer_tgt, device\n",
        "\n",
        "def generate(model, tokenizer_src, tokenizer_tgt, src_text, device, max_length=350):\n",
        "    model.eval()\n",
        "\n",
        "    enc_input = tokenizer_src.encode(src_text).ids\n",
        "    enc_input = torch.tensor([tokenizer_src.token_to_id('[SOS]')] + enc_input + [tokenizer_src.token_to_id('[EOS]')]).unsqueeze(0)\n",
        "\n",
        "    enc_mask = (enc_input != tokenizer_src.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int()\n",
        "\n",
        "    enc_input = enc_input.to(device)\n",
        "    enc_mask = enc_mask.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_output = model.encode(enc_input, enc_mask)\n",
        "        dec_input = torch.tensor([[tokenizer_tgt.token_to_id('[SOS]')]]).to(device)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            dec_mask = causal_mask(dec_input.size(1)).to(device)\n",
        "\n",
        "            dec_output = model.decode(enc_output, enc_mask, dec_input, dec_mask)\n",
        "            proj_output = model.project(dec_output)\n",
        "\n",
        "            next_word = proj_output[:, -1].argmax(dim=-1)\n",
        "            dec_input = torch.cat([dec_input, next_word.unsqueeze(-1)], dim=1)\n",
        "\n",
        "            if next_word.item() == tokenizer_tgt.token_to_id('[EOS]'):\n",
        "                break\n",
        "\n",
        "    translated_tokens = [tokenizer_tgt.id_to_token(t.item()) for t in dec_input[0]]\n",
        "    translated_text = ' '.join([t for t in translated_tokens if t not in ['[SOS]', '[EOS]', '[PAD]']])\n",
        "\n",
        "    return translated_text\n",
        "\n",
        "\n",
        "model, tokenizer_src, tokenizer_tgt, device = load_model(config)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-B1hXBPhsd4p",
        "outputId": "27918e29-1c37-402f-f609-55d039117329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English to Italian Translations:\n",
            "--------------------------------------------------\n",
            "EN: the weather is beautiful today\n",
            "IT: Il giorno Ã¨ molto contento .\n",
            "--------------------------------------------------\n",
            "EN: how are you?\n",
            "IT: Come siete cosÃ¬ ?\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "test_sentences = [\n",
        "        \"the weather is beautiful today\",\n",
        "        \"how are you?\"\n",
        "    ]\n",
        "\n",
        "print(\"English to Italian Translations:\")\n",
        "print(\"-\" * 50)\n",
        "for sentence in test_sentences:\n",
        "    translation = generate(model, tokenizer_src, tokenizer_tgt, sentence, device)\n",
        "    print(f\"EN: {sentence}\")\n",
        "    print(f\"IT: {translation}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Cofqp53bQB"
      },
      "source": [
        "#### Acknowledgements\n",
        "\n",
        "This notebook is heavily adapted from a number of incredible resources on Transformers, including but not limited to:\n",
        "\n",
        "- https://blog.floydhub.com/the-transformer-in-pytorch/\n",
        "- https://arxiv.org/pdf/1706.03762.pdf\n",
        "- https://txt.cohere.com/what-are-transformer-models/\n",
        "- https://jalammar.github.io/illustrated-transformer/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c374a73f8b044ccaf1496600033f4ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23d9390da58c40c9960d4061d738281a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ebde74bdf0f47bc8f0e765c923d0617": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30fdc2dcc66e45249784d589fc3f6fe4",
            "max": 28064,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3bed5d7ec2b4e4592e53737680df900",
            "value": 28064
          }
        },
        "3051ca72ff394a43b13dd6d3c464d8ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30fdc2dcc66e45249784d589fc3f6fe4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3159e6a7df704ced88e0f7cc827317ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35e03fc3caca4f598b58596284a97268": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f715be0422440a385277b9ac3281166": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23d9390da58c40c9960d4061d738281a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5d8d4f2bea7d48179265bd04fe67e0de",
            "value": "train-00000-of-00001.parquet:â€‡100%"
          }
        },
        "450369d045444bd882f937a2e3360633": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6870c43752c64d28aa4719f9040adf2e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5000d2f20c8940ccbb48882ec044f794",
            "value": "README.md:â€‡100%"
          }
        },
        "4688b7bcc6be45d1a11a2bd6de4f5e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f97814858bac42bd893911fe8e7299e3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3051ca72ff394a43b13dd6d3c464d8ee",
            "value": "â€‡32332/32332â€‡[00:00&lt;00:00,â€‡304120.20â€‡examples/s]"
          }
        },
        "5000d2f20c8940ccbb48882ec044f794": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d8d4f2bea7d48179265bd04fe67e0de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6085009c74a6457399a8ac54ce72727e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d7e11b9cbec499388135ba6eb4dfcdd",
              "IPY_MODEL_cc7141a44ec44a66994c4b3a3386df6b",
              "IPY_MODEL_4688b7bcc6be45d1a11a2bd6de4f5e27"
            ],
            "layout": "IPY_MODEL_ed08816530a94263a8d10890d4514c16"
          }
        },
        "6870c43752c64d28aa4719f9040adf2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d7e11b9cbec499388135ba6eb4dfcdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8475c0c2a5b48c98088ee59998dde99",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ef4db5c750354ea796c24f3085a83325",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "7c2499083c47434fbe08b91c51c8a043": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9d0952781ec4d7bae15e7676c5288ac",
            "max": 5726189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4361021b3194e5fb9c2eb05cc390310",
            "value": 5726189
          }
        },
        "7c34fa2e87e145a2a081157b53461876": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c374a73f8b044ccaf1496600033f4ae",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3159e6a7df704ced88e0f7cc827317ae",
            "value": "â€‡28.1k/28.1kâ€‡[00:00&lt;00:00,â€‡2.14MB/s]"
          }
        },
        "7c7890ba428744a8b73ca27d4df978dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a7cb2f19965446b8a0e4a146458b687": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f715be0422440a385277b9ac3281166",
              "IPY_MODEL_7c2499083c47434fbe08b91c51c8a043",
              "IPY_MODEL_914bd3f816e44a2dbc752dd03785a75e"
            ],
            "layout": "IPY_MODEL_35e03fc3caca4f598b58596284a97268"
          }
        },
        "8aaae6369779418ab1923cf678182884": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "914bd3f816e44a2dbc752dd03785a75e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aaae6369779418ab1923cf678182884",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b08f48cdd7ec4d9d831ebb71d63d8b9c",
            "value": "â€‡5.73M/5.73Mâ€‡[00:00&lt;00:00,â€‡42.7MB/s]"
          }
        },
        "9691ca93db0846d58e5f69ab63de8e27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3bed5d7ec2b4e4592e53737680df900": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8475c0c2a5b48c98088ee59998dde99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b08f48cdd7ec4d9d831ebb71d63d8b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4361021b3194e5fb9c2eb05cc390310": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc7141a44ec44a66994c4b3a3386df6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9691ca93db0846d58e5f69ab63de8e27",
            "max": 32332,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dcf3b34b34164aa493154bb080d42dd9",
            "value": 32332
          }
        },
        "d9d0952781ec4d7bae15e7676c5288ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcf3b34b34164aa493154bb080d42dd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed08816530a94263a8d10890d4514c16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef4db5c750354ea796c24f3085a83325": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f97814858bac42bd893911fe8e7299e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa87247f7e2a4f8db2fa81d24479503b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_450369d045444bd882f937a2e3360633",
              "IPY_MODEL_2ebde74bdf0f47bc8f0e765c923d0617",
              "IPY_MODEL_7c34fa2e87e145a2a081157b53461876"
            ],
            "layout": "IPY_MODEL_7c7890ba428744a8b73ca27d4df978dc"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
