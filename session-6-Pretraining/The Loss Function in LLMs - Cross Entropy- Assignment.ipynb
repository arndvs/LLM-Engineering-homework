{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1gO5Y8QTvF7b2BpyYIXlSh7lMTOCkDtPT","timestamp":1733867968791},{"file_id":"1VEk0mdGYKfPezWDJ4ErajNNlr4pq2Duk","timestamp":1733438297454}],"machine_shape":"hm","gpuType":"L4","collapsed_sections":["otLV55h9T322"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# The Loss Function in LLMs - Cross Entropy- AIMS\n","\n","- Breakout #1:\n","  - Task 1: Dependencies\n","  - Task 2: Data Preparation\n","    - 🏗️ Activity #1\n","    - ❓ Question #1\n","  - Task 3: Training Loop\n","    - 👪❓ Discussion Question #1\n","  - Task 4: Training the Model\n","    - ❓ Question #2\n","\n","Now that we have a better understanding of what decoder-only transformer based LLMs are doing to predict the next token, let's look at how they train using that prediciton mechanism!\n","\n","> ⚠ NOTE: This notebook is **NOT** compatible with the T4 Instance. Please ensure you're in the L4 or A100 instance."],"metadata":{"id":"4QBs8FHvShu6"}},{"cell_type":"markdown","source":["## Task 1: Dependencies\n","\n","We'll start by loading the repository and the requirements not included in Colab by default."],"metadata":{"id":"-Qtr6wnLTYlR"}},{"cell_type":"code","source":["!pip install -qU datasets tiktoken wandb tqdm triton"],"metadata":{"id":"XcfnV1C3Cz50","executionInfo":{"status":"ok","timestamp":1733868042905,"user_tz":480,"elapsed":23493,"user":{"displayName":"Aaron Davis","userId":"00482616874989046338"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c0a852f1-93be-4fbd-af73-d1c02cf53454"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4n9tJuC0BKFJ","executionInfo":{"status":"ok","timestamp":1733868173051,"user_tz":480,"elapsed":1213,"user":{"displayName":"Aaron Davis","userId":"00482616874989046338"}},"outputId":"d1351834-7e81-4367-8beb-26ec66b0bce4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'nanoGPT'...\n","remote: Enumerating objects: 686, done.\u001b[K\n","remote: Counting objects: 100% (4/4), done.\u001b[K\n","remote: Compressing objects: 100% (4/4), done.\u001b[K\n","remote: Total 686 (delta 0), reused 2 (delta 0), pack-reused 682 (from 1)\u001b[K\n","Receiving objects: 100% (686/686), 959.50 KiB | 3.03 MiB/s, done.\n","Resolving deltas: 100% (385/385), done.\n"]}],"source":["!git clone https://github.com/karpathy/nanoGPT.git"]},{"cell_type":"code","source":["%cd nanoGPT"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ucLa1gp3CmB0","executionInfo":{"status":"ok","timestamp":1733868175341,"user_tz":480,"elapsed":169,"user":{"displayName":"Aaron Davis","userId":"00482616874989046338"}},"outputId":"4f58d0ca-fe69-4430-c2c8-0ee71bb49d2c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/nanoGPT\n"]}]},{"cell_type":"markdown","source":["## Task 2: Data Preparation\n","\n","In order to have the correct form of our inputs (tokens) we need to prepare our dataset - let's do this using the script provided by the repository!"],"metadata":{"id":"kstygtQwVOUZ"}},{"cell_type":"code","source":["!python data/shakespeare/prepare.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PNV0Z2uLVVay","executionInfo":{"status":"ok","timestamp":1733868181223,"user_tz":480,"elapsed":2874,"user":{"displayName":"Aaron Davis","userId":"00482616874989046338"}},"outputId":"faf3c4f2-4bf6-4e65-a85f-99dd56bb1613"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["train has 301,966 tokens\n","val has 36,059 tokens\n"]}]},{"cell_type":"markdown","source":["##### 🏗️ Activity #1:\n","\n","Describe what is happening in the `prepare.py` function in the [repository](https://github.com/karpathy/nanoGPT/blob/master/data/shakespeare/prepare.py) in natural language."],"metadata":{"id":"OFeTcZOV_a4v"}},{"cell_type":"markdown","source":["⚓\n","\n","1. Downloads Shakespeare's complete works if not already present\n","2. Reads and processes the text data:\n","\n","  * Splits it into train (90%) and validation (10%) sets\n","  * Uses GPT-2's tokenizer to encode the text into tokens\n","  * Saves the tokenized data as binary files for efficient loading\n","\n","\n","4. The resulting tokens are then saved as train.bin and val.bin files with corresponding token counts printed (train has ~302k tokens, val has ~36k tokens)"],"metadata":{"id":"rloZnl7mwgl-"}},{"cell_type":"code","source":["import os\n","import requests\n","import tiktoken\n","import numpy as np\n","\n","# Define the path for the input file in the same directory as this script\n","input_file_path = os.path.join(os.path.dirname(__file__), 'input.txt')\n","\n","# Download the dataset if it doesn't exist locally\n","if not os.path.exists(input_file_path):\n","    # URL for the tiny Shakespeare dataset from Karpathy's repo\n","    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n","    # Download and write the data to input.txt\n","    with open(input_file_path, 'w', encoding='utf-8') as f:\n","        f.write(requests.get(data_url).text)\n","\n","# Read the downloaded dataset\n","with open(input_file_path, 'r', encoding='utf-8') as f:\n","    data = f.read()\n","\n","# Get total length of the dataset\n","n = len(data)\n","\n","# Split data into training (90%) and validation (10%) sets\n","train_data = data[:int(n*0.9)]\n","val_data = data[int(n*0.9):]\n","\n","# Initialize the GPT-2 tokenizer from tiktoken\n","enc = tiktoken.get_encoding(\"gpt2\")\n","\n","# Convert text to tokens using GPT-2's byte-pair encoding\n","train_ids = enc.encode_ordinary(train_data)\n","val_ids = enc.encode_ordinary(val_data)\n","\n","# Print token counts for both sets\n","print(f\"train has {len(train_ids):,} tokens\")\n","print(f\"val has {len(val_ids):,} tokens\")\n","\n","# Convert token lists to numpy arrays with 16-bit unsigned integers\n","# uint16 is used because GPT-2's vocabulary size is less than 2^16\n","train_ids = np.array(train_ids, dtype=np.uint16)\n","val_ids = np.array(val_ids, dtype=np.uint16)\n","\n","# Save arrays as binary files for efficient storage and loading\n","# Binary files are faster to load than text files during training\n","train_ids.tofile(os.path.join(os.path.dirname(__file__), 'train.bin'))\n","val_ids.tofile(os.path.join(os.path.dirname(__file__), 'val.bin'))\n","\n","# Final token counts in the binary files:\n","# train.bin has 301,966 tokens\n","# val.bin has 36,059 tokens"],"metadata":{"id":"061Nb2eTwQjw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### ❓ Question #1:\n","\n","What kind of tokenization strategy is being used here? (provide some examples of tokens)"],"metadata":{"id":"yv7GVHU3_2_k"}},{"cell_type":"markdown","source":["⚓\n","\n","The tokenization strategy being used is the GPT-2 tokenizer (via tiktoken library), which uses byte-pair encoding (BPE). This creates subword tokens that can represent common word pieces efficiently.\n","\n","\n","\n","```\n","# Initialize the GPT-2 tokenizer from tiktoken\n","enc = tiktoken.get_encoding(\"gpt2\")\n","```\n","\n","\n","The tokenizer breaks this into subword units like:\n","\n","* Common words: \"Prince\", \"with\", \"his\"\n","* Subword pieces: \"zel\", \"enes\"\n","* Special tokens: \"\\n\" for newlines\n","* Punctuation: \",\""],"metadata":{"id":"hc6d2tK-xGPF"}},{"cell_type":"markdown","source":["## Training Loop\n","\n","We'll leverage the training loop from Karpathy's repository to focus in on the specifics of where we're using loss - how we're using it - and what it means!"],"metadata":{"id":"hPyDaUl3TgSg"}},{"cell_type":"markdown","source":["### Preamble Code"],"metadata":{"id":"otLV55h9T322"}},{"cell_type":"code","source":["import os\n","import time\n","import math\n","import pickle\n","from contextlib import nullcontext\n","\n","import numpy as np\n","import torch\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","from torch.distributed import init_process_group, destroy_process_group\n","\n","from model import GPTConfig, GPT\n","\n","# -----------------------------------------------------------------------------\n","# default config values designed to train a gpt2 (124M) on OpenWebText\n","# I/O\n","out_dir = 'out'\n","eval_interval = 2000\n","log_interval = 1\n","eval_iters = 200\n","eval_only = False # if True, script exits right after the first eval\n","always_save_checkpoint = True # if True, always save a checkpoint after each eval\n","init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n","# wandb logging\n","wandb_log = False # disabled by default\n","wandb_project = 'owt'\n","wandb_run_name = 'gpt2' # 'run' + str(time.time())\n","# data\n","dataset = 'shakespeare'\n","gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n","batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n","block_size = 1024\n","# model\n","n_layer = 12\n","n_head = 12\n","n_embd = 768\n","dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n","bias = False # do we use bias inside LayerNorm and Linear layers?\n","# adamw optimizer\n","learning_rate = 6e-4 # max learning rate\n","max_iters = 10 # total number of training iterations\n","weight_decay = 1e-1\n","beta1 = 0.9\n","beta2 = 0.95\n","grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n","# learning rate decay settings\n","decay_lr = True # whether to decay the learning rate\n","warmup_iters = 10 # how many steps to warm up for\n","lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n","min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n","# DDP settings\n","backend = 'nccl' # 'nccl', 'gloo', etc.\n","# system\n","device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n","dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n","compile = True # use PyTorch 2.0 to compile the model to be faster\n","# -----------------------------------------------------------------------------\n","config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n","config = {k: globals()[k] for k in config_keys} # will be useful for logging\n","# -----------------------------------------------------------------------------\n","\n","# various inits, derived attributes, I/O setup\n","ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n","if ddp:\n","    init_process_group(backend=backend)\n","    ddp_rank = int(os.environ['RANK'])\n","    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n","    ddp_world_size = int(os.environ['WORLD_SIZE'])\n","    device = f'cuda:{ddp_local_rank}'\n","    torch.cuda.set_device(device)\n","    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n","    seed_offset = ddp_rank # each process gets a different seed\n","    # world_size number of processes will be training simultaneously, so we can scale\n","    # down the desired gradient accumulation iterations per process proportionally\n","    assert gradient_accumulation_steps % ddp_world_size == 0\n","    gradient_accumulation_steps //= ddp_world_size\n","else:\n","    # if not ddp, we are running on a single gpu, and one process\n","    master_process = True\n","    seed_offset = 0\n","    ddp_world_size = 1\n","tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n","print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n","\n","if master_process:\n","    os.makedirs(out_dir, exist_ok=True)\n","torch.manual_seed(1337 + seed_offset)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n","# note: float16 data type will automatically use a GradScaler\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n","\n","# poor man's data loader\n","data_dir = os.path.join('data', dataset)\n","def get_batch(split):\n","    # We recreate np.memmap every batch to avoid a memory leak, as per\n","    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n","    if split == 'train':\n","        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n","    else:\n","        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n","    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n","    if device_type == 'cuda':\n","        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n","        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n","    else:\n","        x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n","iter_num = 0\n","best_val_loss = 1e9\n","\n","# attempt to derive vocab_size from the dataset\n","meta_path = os.path.join(data_dir, 'meta.pkl')\n","meta_vocab_size = None\n","if os.path.exists(meta_path):\n","    with open(meta_path, 'rb') as f:\n","        meta = pickle.load(f)\n","    meta_vocab_size = meta['vocab_size']\n","    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n","\n","# model init\n","model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n","                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n","if init_from == 'scratch':\n","    # init a new model from scratch\n","    print(\"Initializing a new model from scratch\")\n","    # determine the vocab size we'll use for from-scratch training\n","    if meta_vocab_size is None:\n","        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n","    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n","    gptconf = GPTConfig(**model_args)\n","    model = GPT(gptconf)\n","elif init_from == 'resume':\n","    print(f\"Resuming training from {out_dir}\")\n","    # resume training from a checkpoint.\n","    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n","    checkpoint = torch.load(ckpt_path, map_location=device)\n","    checkpoint_model_args = checkpoint['model_args']\n","    # force these config attributes to be equal otherwise we can't even resume training\n","    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n","    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n","        model_args[k] = checkpoint_model_args[k]\n","    # create the model\n","    gptconf = GPTConfig(**model_args)\n","    model = GPT(gptconf)\n","    state_dict = checkpoint['model']\n","    # fix the keys of the state dictionary :(\n","    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n","    unwanted_prefix = '_orig_mod.'\n","    for k,v in list(state_dict.items()):\n","        if k.startswith(unwanted_prefix):\n","            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","    model.load_state_dict(state_dict)\n","    iter_num = checkpoint['iter_num']\n","    best_val_loss = checkpoint['best_val_loss']\n","elif init_from.startswith('gpt2'):\n","    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n","    # initialize from OpenAI GPT-2 weights\n","    override_args = dict(dropout=dropout)\n","    model = GPT.from_pretrained(init_from, override_args)\n","    # read off the created config params, so we can store them into checkpoint correctly\n","    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n","        model_args[k] = getattr(model.config, k)\n","# crop down the model block size if desired, using model surgery\n","if block_size < model.config.block_size:\n","    model.crop_block_size(block_size)\n","    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n","model.to(device)\n","\n","# initialize a GradScaler. If enabled=False scaler is a no-op\n","scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","\n","# optimizer\n","optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n","if init_from == 'resume':\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","checkpoint = None # free up memory\n","\n","# compile the model\n","if compile:\n","    print(\"compiling the model... (takes a ~minute)\")\n","    unoptimized_model = model\n","    model = torch.compile(model) # requires PyTorch 2.0\n","\n","# wrap model into DDP container\n","if ddp:\n","    model = DDP(model, device_ids=[ddp_local_rank])\n","\n","# helps estimate an arbitrarily accurate loss over either split using many batches\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            with ctx:\n","                logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","# learning rate decay scheduler (cosine with warmup)\n","def get_lr(it):\n","    # 1) linear warmup for warmup_iters steps\n","    if it < warmup_iters:\n","        return learning_rate * it / warmup_iters\n","    # 2) if it > lr_decay_iters, return min learning rate\n","    if it > lr_decay_iters:\n","        return min_lr\n","    # 3) in between, use cosine decay down to min learning rate\n","    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n","    return min_lr + coeff * (learning_rate - min_lr)\n","\n","# logging\n","if wandb_log and master_process:\n","    import wandb\n","    wandb.init(project=wandb_project, name=wandb_run_name, config=config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QgyIgEn1T3Te","executionInfo":{"status":"ok","timestamp":1733868218005,"user_tz":480,"elapsed":11619,"user":{"displayName":"Aaron Davis","userId":"00482616874989046338"}},"outputId":"094fe9ec-ea0c-4143-a705-acacc0b78220"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["tokens per iteration will be: 12,288\n","Initializing a new model from scratch\n","defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n","number of parameters: 123.59M\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-30430fb971dc>:177: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"]},{"output_type":"stream","name":"stdout","text":["num decayed parameter tensors: 50, with 124,354,560 parameters\n","num non-decayed parameter tensors: 25, with 19,200 parameters\n","using fused AdamW: True\n","compiling the model... (takes a ~minute)\n"]}]},{"cell_type":"markdown","source":["### Task 3: Training Loop\n","\n","Here is where the magic happens!\n","\n","Before we get straight into training - let's look at our model to obtain some key insights."],"metadata":{"id":"MFQcSiOYUyBV"}},{"cell_type":"code","source":["print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"27KCZIhtuRFc","executionInfo":{"status":"ok","timestamp":1733868380235,"user_tz":480,"elapsed":169,"user":{"displayName":"Aaron Davis","userId":"00482616874989046338"}},"outputId":"b7961b7b-5917-4d3c-ad83-d8931954cb61"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["OptimizedModule(\n","  (_orig_mod): GPT(\n","    (transformer): ModuleDict(\n","      (wte): Embedding(50304, 768)\n","      (wpe): Embedding(1024, 768)\n","      (drop): Dropout(p=0.0, inplace=False)\n","      (h): ModuleList(\n","        (0-11): 12 x Block(\n","          (ln_1): LayerNorm()\n","          (attn): CausalSelfAttention(\n","            (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n","            (c_proj): Linear(in_features=768, out_features=768, bias=False)\n","            (attn_dropout): Dropout(p=0.0, inplace=False)\n","            (resid_dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (ln_2): LayerNorm()\n","          (mlp): MLP(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n","            (gelu): GELU(approximate='none')\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","      )\n","      (ln_f): LayerNorm()\n","    )\n","    (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n","  )\n",")\n"]}]},{"cell_type":"markdown","source":["##### 👪❓ Discussion Question #1:\n","\n","Describe how this model is different that the traditional Transformer Architecture from the paper \"Attention is All You Need\"."],"metadata":{"id":"piujJIlnBK22"}},{"cell_type":"markdown","source":["Notice our final layer - the `lm_head` - and how it has `out_features=50304`. This number of output features lines up exactly with our vocabulary! When we're making predictions, we're making predictions about which token (in our vocabulary) should be selected next!"],"metadata":{"id":"NS1oTMdV-oH-"}},{"cell_type":"markdown","source":["⚓\n","\n","This model differs from the traditional Transformer architecture in \"Attention is All You Need\" in several key ways:\n","\n","1. Decoder-Only: It only uses the decoder portion of the transformer, removing the encoder entirely. The original transformer had both encoder and decoder stacks.\n","2. Scale: This uses 12 decoder blocks instead of the original paper's 6 encoder + 6 decoder blocks setup.\n","3. Casual Self-Attention: Uses masked/causal attention to prevent looking at future tokens, while the original allowed bidirectional attention in the encoder.\n","4. No Cross-Attention: Since there's no encoder, there's no encoder-decoder cross attention mechanism.\n","5. Training Objective: Trained purely on next token prediction (causal language modeling) rather than the original's encoder-decoder translation task."],"metadata":{"id":"HNXIhuRoxcQs"}},{"cell_type":"markdown","source":["#### First Batch - What is a Batch?\n","\n","Let's look at our first batch to see what exactly a \"batch\" is in this context."],"metadata":{"id":"NX778-01U2SF"}},{"cell_type":"code","source":["X, Y = get_batch('train') # fetch the very first batch"],"metadata":{"id":"Gd6LWL9LU14w","executionInfo":{"status":"ok","timestamp":1733868404937,"user_tz":480,"elapsed":192,"user":{"displayName":"Aaron Davis","userId":"00482616874989046338"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print(f\"X Shape: {X.shape}, Y Shape: {Y.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6cC2iGj-q8i","executionInfo":{"status":"ok","timestamp":1733868408668,"user_tz":480,"elapsed":272,"user":{"displayName":"Aaron Davis","userId":"00482616874989046338"}},"outputId":"e8f2f79b-9b89-4b66-93e2-9fbe2d64c753"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["X Shape: torch.Size([12, 1024]), Y Shape: torch.Size([12, 1024])\n"]}]},{"cell_type":"markdown","source":["Let's look at what our X and Y look like!\n","\n","> NOTE: We'll only look at the last 5 tokens of the last batch to get a sense of what is happening under the hood of the data selection."],"metadata":{"id":"juIxTM6HVrfk"}},{"cell_type":"code","source":["print(X[:][-1][-5:])\n","print(Y[:][-1][-5:])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lThD-QrrVqlK","executionInfo":{"status":"ok","timestamp":1733868414536,"user_tz":480,"elapsed":205,"user":{"displayName":"Aaron Davis","userId":"00482616874989046338"}},"outputId":"3889c933-a39a-48fb-b965-9172433b5523"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([18719,    11,   351,   465, 21752], device='cuda:0')\n","tensor([   11,   351,   465, 21752,    11], device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["Notice how X and Y are simply shifted by a single index - where Y contains (at every index) the token that *follows* X!\n","\n","So essentially - Y contains the *labels* (or target) for X!\n","\n","Let's see how we can leverage this in our training loop!\n","\n","Before we start training - let's import our decoder so we can see specific text that our model is leveraging!"],"metadata":{"id":"fWgmihsgWBmN"}},{"cell_type":"code","source":["import tiktoken\n","\n","enc = tiktoken.get_encoding(\"gpt2\")\n","encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n","decode = lambda l: enc.decode(l)"],"metadata":{"id":"bL8uueRjyqL6","executionInfo":{"status":"ok","timestamp":1733868418350,"user_tz":480,"elapsed":386,"user":{"displayName":"Aaron Davis","userId":"00482616874989046338"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## Task 4: Training the Model"],"metadata":{"id":"-Hk5EmQjB3Gb"}},{"cell_type":"markdown","source":["The training loop (provided through the NanoGPT repository) has a lot of interesting things going on - but we're going to focus on the specific section of the training loop that relates to the loss and logits."],"metadata":{"id":"xQXAoLD19w4g"}},{"cell_type":"code","source":["t0 = time.time()\n","local_iter_num = 0\n","raw_model = model.module if ddp else model\n","running_mfu = -1.0\n","while True:\n","    lr = get_lr(iter_num) if decay_lr else learning_rate\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","    if iter_num % eval_interval == 0 and master_process:\n","        losses = estimate_loss()\n","        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        if wandb_log:\n","            wandb.log({\n","                \"iter\": iter_num,\n","                \"train/loss\": losses['train'],\n","                \"val/loss\": losses['val'],\n","                \"lr\": lr,\n","                \"mfu\": running_mfu*100,\n","            })\n","        if losses['val'] < best_val_loss or always_save_checkpoint:\n","            best_val_loss = losses['val']\n","            if iter_num > 0:\n","                checkpoint = {\n","                    'model': raw_model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'model_args': model_args,\n","                    'iter_num': iter_num,\n","                    'best_val_loss': best_val_loss,\n","                    'config': config,\n","                }\n","                print(f\"saving checkpoint to {out_dir}\")\n","                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n","    if iter_num == 0 and eval_only:\n","        break\n","    for micro_step in range(gradient_accumulation_steps):\n","        if ddp:\n","            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n","        ####### LOGITS AND LOSS #######\n","        with ctx:\n","            logits, loss = model(X, Y)\n","            print(f\"Our inputs (truncated to the last 15 tokens in the last batch) were: {X[:][-1][-15:].cpu().numpy()}\")\n","            print(f\"Represented in text that is: {[decode(X[:][-1][-15:].cpu().numpy())]}\")\n","            print(f\"Our targets represented in text were: {[decode(Y[:][-1][-15:].cpu().numpy())]}\")\n","            print(f\"Our logits are in shape: {logits.shape}\")\n","            print(f\"The Vocabulary Size is: {logits.shape[-1]}\")\n","            print(f\"The Sequence Length is: {logits.shape[1]}\")\n","            print(f\"Our loss was calculated as: {loss}\")\n","            loss = loss / gradient_accumulation_steps\n","        X, Y = get_batch('train')\n","        scaler.scale(loss).backward()\n","        ###############################\n","    if grad_clip != 0.0:\n","        scaler.unscale_(optimizer)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","    scaler.step(optimizer)\n","    scaler.update()\n","    optimizer.zero_grad(set_to_none=True)\n","    t1 = time.time()\n","    dt = t1 - t0\n","    t0 = t1\n","    if iter_num % log_interval == 0 and master_process:\n","        lossf = loss.item() * gradient_accumulation_steps\n","        if local_iter_num >= 5:\n","            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n","            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n","        print(f\"iter {iter_num}: time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n","    iter_num += 1\n","    local_iter_num += 1\n","    if iter_num > max_iters:\n","        break\n","if ddp:\n","    destroy_process_group()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AhuKbQE6Upm0","executionInfo":{"status":"ok","timestamp":1733868510477,"user_tz":480,"elapsed":80913,"user":{"displayName":"Aaron Davis","userId":"00482616874989046338"}},"outputId":"45ee56cd-86f5-4afd-eacd-a757a2975455"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["step 0: train loss 11.0040, val loss 10.9976\n","Our inputs (truncated to the last 15 tokens in the last batch) were: [ 9005  4432   528   417    11   198 31056   286  2165   844 18719    11\n","   351   465 21752]\n","Represented in text that is: [' Prince Florizel,\\nSon of Polixenes, with his princess']\n","Our targets represented in text were: [' Florizel,\\nSon of Polixenes, with his princess,']\n","Our logits are in shape: torch.Size([12, 1024, 50304])\n","The Vocabulary Size is: 50304\n","The Sequence Length is: 1024\n","Our loss was calculated as: 10.98572063446045\n","iter 0: time 77611.41ms, mfu -100.00%\n","Our inputs (truncated to the last 15 tokens in the last batch) were: [   25   198 33873  3183    11  1497   351   683     0   198   198    51\n"," 38409    25   198]\n","Represented in text that is: [':\\nSoldiers, away with him!\\n\\nTutor:\\n']\n","Our targets represented in text were: ['\\nSoldiers, away with him!\\n\\nTutor:\\nAh']\n","Our logits are in shape: torch.Size([12, 1024, 50304])\n","The Vocabulary Size is: 50304\n","The Sequence Length is: 1024\n","Our loss was calculated as: 11.003988265991211\n","iter 1: time 114.93ms, mfu -100.00%\n","Our inputs (truncated to the last 15 tokens in the last batch) were: [4776  502  510  329  262  198 3157  395  638 1015  287 1951  437  296\n","   13]\n","Represented in text that is: [' score me up for the\\nlyingest knave in Christendom.']\n","Our targets represented in text were: [' me up for the\\nlyingest knave in Christendom. What']\n","Our logits are in shape: torch.Size([12, 1024, 50304])\n","The Vocabulary Size is: 50304\n","The Sequence Length is: 1024\n","Our loss was calculated as: 9.737184524536133\n","iter 2: time 312.49ms, mfu -100.00%\n","Our inputs (truncated to the last 15 tokens in the last batch) were: [ 198   54 1670  290  649 1494 1549   13  198  198 4805 1268 5222   25\n","  198]\n","Represented in text that is: [\"\\nWarm and new kill'd.\\n\\nPRINCE:\\n\"]\n","Our targets represented in text were: [\"Warm and new kill'd.\\n\\nPRINCE:\\nSearch\"]\n","Our logits are in shape: torch.Size([12, 1024, 50304])\n","The Vocabulary Size is: 50304\n","The Sequence Length is: 1024\n","Our loss was calculated as: 9.323972702026367\n","iter 3: time 308.00ms, mfu -100.00%\n","Our inputs (truncated to the last 15 tokens in the last batch) were: [  674 10715    13   198   198 15946   455    25   198  5247   284    11\n"," 15967    26   345]\n","Represented in text that is: [' our mystery.\\n\\nProvost:\\nGo to, sir; you']\n","Our targets represented in text were: [' mystery.\\n\\nProvost:\\nGo to, sir; you weigh']\n","Our logits are in shape: torch.Size([12, 1024, 50304])\n","The Vocabulary Size is: 50304\n","The Sequence Length is: 1024\n","Our loss was calculated as: 9.514644622802734\n","iter 4: time 324.95ms, mfu -100.00%\n","Our inputs (truncated to the last 15 tokens in the last batch) were: [  11 4379  705 4246  292  339  326  925  345  284 1207  577   11  198\n"," 7120]\n","Represented in text that is: [\", seeing 'twas he that made you to depose,\\nYour\"]\n","Our targets represented in text were: [\" seeing 'twas he that made you to depose,\\nYour oath\"]\n","Our logits are in shape: torch.Size([12, 1024, 50304])\n","The Vocabulary Size is: 50304\n","The Sequence Length is: 1024\n","Our loss was calculated as: 8.966315269470215\n","iter 5: time 310.24ms, mfu 10.85%\n","Our inputs (truncated to the last 15 tokens in the last batch) were: [   43  2885    56 20176 24212    51    25   198   464   661   287   262\n","  4675  3960 43989]\n","Represented in text that is: ['LADY CAPULET:\\nThe people in the street cry Romeo']\n","Our targets represented in text were: ['ADY CAPULET:\\nThe people in the street cry Romeo,']\n","Our logits are in shape: torch.Size([12, 1024, 50304])\n","The Vocabulary Size is: 50304\n","The Sequence Length is: 1024\n","Our loss was calculated as: 8.681790351867676\n","iter 6: time 313.41ms, mfu 10.84%\n","Our inputs (truncated to the last 15 tokens in the last batch) were: [ 2236  1282   284 22363  3725    11   326   198  1639  4145   423  7715\n","  1549   502     0]\n","Represented in text that is: [\" shall come to clearer knowledge, that\\nYou thus have publish'd me!\"]\n","Our targets represented in text were: [\" come to clearer knowledge, that\\nYou thus have publish'd me! Gentle\"]\n","Our logits are in shape: torch.Size([12, 1024, 50304])\n","The Vocabulary Size is: 50304\n","The Sequence Length is: 1024\n","Our loss was calculated as: 8.433615684509277\n","iter 7: time 321.27ms, mfu 10.80%\n","Our inputs (truncated to the last 15 tokens in the last batch) were: [  262  1633   198  1870   307   407  4259  1549   287 27666 29079    11\n","   198    39  2502]\n","Represented in text that is: [\" the air\\nAnd be not fix'd in doom perpetual,\\nHover\"]\n","Our targets represented in text were: [\" air\\nAnd be not fix'd in doom perpetual,\\nHover about\"]\n","Our logits are in shape: torch.Size([12, 1024, 50304])\n","The Vocabulary Size is: 50304\n","The Sequence Length is: 1024\n","Our loss was calculated as: 8.148568153381348\n","iter 8: time 323.62ms, mfu 10.76%\n","Our inputs (truncated to the last 15 tokens in the last batch) were: [ 3025  8557  7739   550    11   198  2484   439  2245   393 26724   502\n","    13  8192   314]\n","Represented in text that is: [' whose spiritual counsel had,\\nShall stop or spur me. Have I']\n","Our targets represented in text were: [' spiritual counsel had,\\nShall stop or spur me. Have I done']\n","Our logits are in shape: torch.Size([12, 1024, 50304])\n","The Vocabulary Size is: 50304\n","The Sequence Length is: 1024\n","Our loss was calculated as: 7.851352691650391\n","iter 9: time 316.33ms, mfu 10.75%\n","Our inputs (truncated to the last 15 tokens in the last batch) were: [  407   257  1573   286  8716    30   198  4366  4467    11 15849    13\n","   198   198    45]\n","Represented in text that is: [' not a word of joy?\\nSome comfort, nurse.\\n\\nN']\n","Our targets represented in text were: [' a word of joy?\\nSome comfort, nurse.\\n\\nNurse']\n","Our logits are in shape: torch.Size([12, 1024, 50304])\n","The Vocabulary Size is: 50304\n","The Sequence Length is: 1024\n","Our loss was calculated as: 7.475807189941406\n","iter 10: time 318.94ms, mfu 10.73%\n"]}]},{"cell_type":"markdown","source":["##### ❓ Question #2:\n","\n","Describe if, and why, this process is supervised or unsupervised."],"metadata":{"id":"Vfg_B5rWB6te"}},{"cell_type":"markdown","source":["⚓\n","\n","This process is unsupervised learning.\n","\n","It's like finishing a phrase, where you naturally know what comes next:\n","\n","\"Twinkle twinkle little ___\"\n","Without anyone teaching it, the model learns by seeing what word naturally follows in text:\n","\n","1. Gets shown: \"Twinkle twinkle little\"\n","2. Has to guess the next word\n","3. Checks that \"star\" actually comes next\n","4. Learns from how right or wrong it was\n","\n","Just like how we learned to complete \"Twinkle twinkle little star\" from hearing the song many times, the model learns from seeing patterns in text. No human had to label or mark anything as \"correct\" - the song itself teaches what comes next.\n","\n","In the code:\n","\n","* X (input): \"Twinkle twinkle little\"\n","* Y (target): \"twinkle little star\"\n","\n","Each sequence is just shifted by one word, letting the model learn naturally like a child hearing songs and stories repeatedly. That's why it's unsupervised - it learns directly from the raw text, not from human teachers providing correct answers."],"metadata":{"id":"6w7w0BzMxuW9"}}]}